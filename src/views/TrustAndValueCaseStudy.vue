
<template>
<main id="main-content">
    <div class="case-study article container mt-5">
    <div class="row pt-5 justify-content-center">
        <div class="col-lg-10">
            <header>
            <h1>Changing the biggest bets of the program: Trust and Value in federal medical research</h1>
            </header>
            <article>
            
<p class="lead">When researching participant motivation and retention for the All of Us research program, I quickly realized that trust was a major barrier to participation, while value was a key driver of adoption and retention. Understanding this interaction became particularly important to my work to improve program satisfaction and success outcomes.</p>

<h2>Investigating Trust</h2>

<p>To measure and improve trust, I began with a literature review to identify its key facets. I found that trust in medical research and government institutions was closely tied to four factors:</p>
<ul>

<li>Humanity: Does the program feel personal and considerate? Does it prioritize the safety and provacy of participants?</li>

<li>Transparency: Is the process accurate and honest? Is it clear about how the program operates, or how data is handled?</li>

<li>Capability: Is what the program give back to participants valuable? Do participants believe in the program’s competency? Is participation easy?</li>

<li>Reliability: Does the program deliver on its promises? Does it respond to feedback and carry out improvements?</li>
</ul>

<p>I translated these concepts into leading and lagging indicators of trust, cataloging existing measures and identifying gaps. This allowed me to integrate trust assessment into a broader customer experience (CX) strategy, ensuring that participant trust was continuously monitored and improved over time.</p>

<h2>Defining & Measuring Value</h2>

<p>Understanding value required a more complex approach. I conducted a landscape analysis and multiple rounds of qualitative interviews to explore what participants truly found meaningful. I followed up with quantitative surveys to validate these findings and synthesize actionable insights.</p>

<p>One of my key takeaways was that value is not static, but rather highly contextual and changes over time. What a participant values at the start of the program may differ from what they value later.</p>

<p>To quantify this, I adapted a method similar to the Single Ease Question (often used in usability testing) to create a Single Value Question (SVQ). This approach enabled us to measure perceived value at different points in the participant journey, providing a more nuanced and data-driven way to understand evolving motivations.</p>

<h2>Shifting Program Priorities</h2>

<p>By analyzing trust and value metrics, I helped shift the program’s focus away from some of the “big bets” it had been making, some of the assumptions about what participants found valuable, toward what participants actually wanted. My research revealed that some of the program’s existing value propositions weren’t resonating, prompting a realignment of offerings and engagement strategies to better match participant expectations.</p>

<h2>Outcomes & Impact</h2>

<p>This research led to the development of new trust and value metrics that became key indicators of program success. By collecting real participant data, I was able to show which aspects of the program were ineffective and guide improvements toward elements that actually increased trust and perceived value.</p>

<p>Ultimately, these insights helped refine the program’s approach to participant engagement, ensuring that efforts were more data-informed, participant-centered, and impactful in the long run.</p>



            </article>
        </div><!--col-->
    </div><!--row-->

</div><!--contain-->
  </main>
</template>

<style scoped>

    </style>
